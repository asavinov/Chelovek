counting grand total of: rules, records, and other aggregates 
(like in Net Vampire)

fuzzy

like this

clustering rules

clustering data

probabilistic conclusions

goal values

variable and value weights

quantity of information in variables (form of conclusion etc.)

removing reduntant disjunctions (test new disjunctions on redundance, 
i.e., the consequence from others)

building the logical equivalent of the table

decomposition of the table

discretizing attribute values (building new syntax from the old one and 
the knowledge base)

---

Calculation of absolute maximum diring table analysis

Reordering disjunctions to simplify absorption. More general disjunctions 
which have more absorption strength go first.

Boolean variant of the algorithm

Study an influence of the order of variables on the serch time.

Study an influence of different parameters of distiribution on the serch time.
In addition, connect these parameters with some options of the algorithm,
e.g., some order of variables, order of data etc.

Study the issue how to introduce certainty factors (any weights) into rules
(for a whole rule, or for its part) in the case of crisp distributions.
For example, it could be used the property of neighborhood, i.e., we should
look at and investigate neighbors of each point (or some other entity).
Or we could take into account some relation between several rules (if several
rules say one and the same then they are added a little weight). When used
in DSS we have to have a possiblity to order altrnatives. Another interesting
approach consist in that more general rules are assigned stronger certainty
factors, and more specific rules (prime disjunctions found) are weaker (i.e., 
we take into account the rank of rules). In addition, we can take into account
how many rules make a concrete decision. This apporach is based on the fact
that a lot of rules can fire when we make use of inference on prime 
disjunctions.

More efficient (re)ordering methods. Study the time it takes to 
reorder the matrix.

Reordering with copying, i.e., we do not copy the rules to its 
positions and then the whole matrix to new position, but rather 
copy the rules in new matrix.

Memory (rule) copy procedure.


=== DONE ===

Header for sectioned vectors

Using the rank for evaluating the possibility of absorption

Using the number of non-0 components to evaluate the possibility of 
absorption (stronger disjuncts necessary have less non-0 components).

============
exhaustive analysis

No document saving, printing

no error messages

processing only text files through DAO

Ordered tables are processed significantly faster!

Insert | Data Source -> Process | Analysis -> Process | Rule Induction

Insert | Data Source
~~~~~~~~~~~~~~~~~~~~
1. Choose TXT file to load as a table

2. Choose necessary columns to process

schema.ini file may be needed which is generated by ODBC dialog when 
creating the corresponding ODBC data source.


Process | Analysis
~~~~~~~~~~~~~~~~~~
Find the syntactic structure of the table, i.e., the values of 
variables used in the table. The Syntax of the knowledge base 
is formed after this procedure

Max Values -- Maximum number of values of one variable. More 
  values are saved as a Default Value.

Process | Rule Induction
~~~~~~~~~~~~~~~~~~~~~~~~
Find semantic dependencies in the table in the form of rules.

Rank -- maximum number of variables in rules

Max Rules -- Maximum number of rules to generate.

Max New Rules -- Defines the limit for buffer where hypotheses are
  kept. No more patters will be generated.

Goal Variable -- Only the rules involving this variable will be 
  generated. If not set all dependencies will be generated.
